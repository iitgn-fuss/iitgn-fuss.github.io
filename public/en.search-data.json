{"/about/":{"data":{"":"To be added."},"title":"About"},"/interesting-reads/measure-your-code/":{"data":{"":"Measuring code performance accurately is crucial, especially when optimizing existing algorithms or designing new ones. In this post, I’ll share my learnings of measuring code performance using various tools and techniques.\nFor measuring execution time, we can use the following techniques:\nWall Clock Time (clock_gettime) Linux resource usage metrics (getrusage) To measure raw Time Stamp Counter (or, ticks), on x86 architectures we can use:\nRDTSC (Read Time-Stamp Counter) Finally, to pinpoint performance bottlenecks, we can use the Linux perf tool; However, there are two ways to use it:\nperf Command line tool perf_event_open syscall for custom profiling code segments. We will explore each of these methods in detail, with the help of an example C code snippet that we will measure using these techniques.","accuracy-considerations#Accuracy Considerations":"The actual elapsed time will vary based on system load and other factors like CPU frequency scaling, context switching, etc. To get more accurate measurements, consider measuring the code segment multiple times—create CDF plots to visualize the distribution of execution times or use statistical measures like 95% CI (Confidence Interval) mean to report the results.\nFor execution time measurements, it’s also important to minimize the impact of other processes running on the system. Running the measurements on a dedicated machine or using CPU affinity to bind the process to a specific core can help reduce variability.","basic-usage-command-line#Basic Usage (Command Line)":"Let’s say we have two implementations of our function—the original with two separate loops, and an optimized version with a merged loop:\nint sum_of_two_arrays(int *array1, int *array2, int size) { int total_sum = 0; int sum1 = 0, sum2 = 0; for (int i = 0; i \u003c size; i++) { sum1 += array1[i]; } for (int i = 0; i \u003c size; i++) { sum2 += array2[i]; } total_sum = sum1 + sum2; return total_sum; } int sum_of_two_arrays_merged_loop(int *array1, int *array2, int size) { int total_sum = 0; for (int i = 0; i \u003c size; i++) { total_sum += array1[i] + array2[i]; } return total_sum; } We can use perf stat to compare them:\n$ perf stat ./sample Performance counter stats for './sample': 124.56 msec task-clock 2 context-switches 39,074 page-faults 298,234,567 cycles # 2.394 GHz 502,345,678 instructions # 1.68 insn per cycle 50,234,567 branches 123,456 branch-misses # 0.25% of all branches For deeper cache analysis:\n$ perf stat -e cache-references,cache-misses,L1-dcache-loads,L1-dcache-load-misses ./my_benchmark","getrusage#getrusage":"In Linux systems, getrusage is a system call that provides resource usage statistics for the calling process. It is used to measure the resources used by the process, like CPU time, memory usage, etc.\nThe key difference from wall clock time: getrusage tells you how much CPU time your code actually used, not how long you waited for it. If the system was busy with other tasks, wall clock time includes that overhead, but getrusage does not.\nPOSIX.1 specifies getrusage(), but specifies only the fields ru_utime and ru_stime. For our benchmarking purposes, we can use ru_utime to measure the user CPU time (time spent executing your code) and ru_stime for system CPU time (time spent in kernel mode on your behalf). To use getrusage, we need to include the header file.\n#include // Function to measure CPU time in microseconds as a long double static inline long double cputime() { struct rusage rus; getrusage(RUSAGE_SELF, \u0026rus); return rus.ru_utime.tv_sec * 1000000.0L + rus.ru_utime.tv_usec; }","gmpbench-style-averaging#GMPBench Style Averaging":"I personally prefer the averaging strategy of GMPBench to report the mean execution time of a given function. What it does is, it starts with a single iteration of the function and doubles the number of iterations until the total elapsed time exceeds a predefined threshold (e.g., 250 ms). Once the threshold is reached, it calculates the average time per iteration by dividing the total elapsed time by the number of iterations. This approach helps to ensure that the measurements are less affected by transient system load variations. Further, based on this computed mean time, it calculates throughput (operations per second) and reports that as the final performance metric.\nOriginally, GMPBench was designed to benchmark arbitrary-precision arithmetic operations in the GMP library, but the underlying methodology can be applied to measure the performance of any function or code segment.\nBelow you can find three different adapted macros of GMPBench style averaging strategy to measure execution time using clock_gettime, RDTSC, and getrusage respectively:\n// Function to measure the time taken by a function using the rusage system call #define TIME_RUSAGE(t, func) \\ do \\ { \\ long int __t0, __times, __t, __tmp; \\ __times = 1; \\ { \\ func; \\ } \\ do \\ { \\ __times \u003c\u003c= 1; \\ __t0 = cputime(); \\ for (__t = 0; __t \u003c __times; __t++) \\ { \\ func; \\ } \\ __tmp = cputime() - __t0; \\ } while (__tmp \u003c 250000); \\ (t) = (double)__tmp / __times; \\ } while (0) // Function to measure the time taken by a function using the timespec clock_gettime system call #define TIME_TIMESPEC(t, func) \\ do \\ { \\ long int __tmp, __times; \\ struct timespec __t0, __t1; \\ __times = 1; \\ { \\ func; \\ } \\ do \\ { \\ __times \u003c\u003c= 1; \\ __t0 = get_timespec(); \\ for (int __t = 0; __t \u003c __times; __t++) \\ { \\ func; \\ } \\ __t1 = get_timespec(); \\ __tmp = diff_timespec_us(__t0, __t1); \\ } while (__tmp \u003c 250000); \\ (t) = (double)__tmp / __times; \\ } while (0) // Function to measure the time taken by a function using the rdtsc instruction #define TIME_RDTSC(t, func) \\ do \\ { \\ unsigned long long __t0, __t1, __times, __tmp; \\ __times = 1; \\ { \\ func; \\ } \\ do \\ { \\ __times \u003c\u003c= 1; \\ __t0 = measure_rdtsc_start(); \\ for (int __t = 0; __t \u003c __times; __t++) \\ { \\ func; \\ } \\ __t1 = measure_rdtscp_end(); \\ __tmp = __t1 - __t0; \\ } while (__tmp \u003c 700000000); \\ __tmp = __tmp * 0.000357; \\ (t) = (double)(__tmp) / __times; \\ } while (0)","linux-perf#Linux perf":"The perf tool in Linux is a powerful performance analysis tool that can measure various aspects of code performance, including CPU cycles, cache misses, branch mispredictions, and more. It provides a wealth of information but can be complex to use effectively.\nThe key insight: timing measurements tell you how long your code takes, but perf tells you why. If your code is slow because of cache misses, branch mispredictions, or excessive page faults, perf helps you pinpoint the actual areas for optimization. Without this, you’re just guessing at what to optimize.","our-example-function#Our Example Function":"The naive function snippet we will use for measurement is as follows:\nint sum_of_two_arrays(int *array1, int *array2, int size) { int total_sum = 0; int sum1 = 0, sum2 = 0; for (int i = 0; i \u003c size; i++) { sum1 += array1[i]; } for (int i = 0; i \u003c size; i++) { sum2 += array2[i]; } total_sum = sum1 + sum2; return total_sum; }","output#Output":"Measuring time using clock_gettime (timespec)... done! sum = 98901230000 Time taken: 1234567.000000 microseconds","output-1#Output":"Measuring time using rusage... done! sum = 98901230000 Time taken: 1234567.000000 microseconds","output-2#Output":"Measuring time using rdtsc... done! sum = 98901230000 Ticks taken: 3456789012 Time taken: 1234073.08 microseconds","programmatic-access-perf_event_open#Programmatic Access (perf_event_open)":"For measuring specific code segments programmatically, we can use the perf_event_open syscall. This lets you start and stop counters around the exact code you want to measure—which is crucial when you want to compare two functions within the same program.\nHere’s a utility library I use for this purpose:\nperf_utils.h:\n#ifndef PERF_UTILS_H #define PERF_UTILS_H #include #include #include #include #include #include #include #include #define MAX_EVENTS 6 #define CORE_NO -1 extern struct perf_event_attr pe[MAX_EVENTS]; extern int fd[MAX_EVENTS]; extern long long count; extern const char *event_names[MAX_EVENTS]; // Function declarations void initialize_perf(); long perf_event_open(struct perf_event_attr *hw_event, pid_t pid, int cpu, int group_fd, unsigned long flags); void read_perf(long long values[]); void write_perf(FILE *file, long long values[]); void start_perf(); void stop_perf(); #endif // PERF_UTILS_H perf_utils.c:\n#include \"perf_utils.h\" #include struct perf_event_attr pe[MAX_EVENTS]; int fd[MAX_EVENTS]; long long count; const char *event_names[MAX_EVENTS] = { \"CPU_CYCLES\", // CPU Cycles \"USER_INSNS\", // User Instructions \"KERN_INSNS\", // Kernel Instructions \"PAGE_FAULTS\", // Page Faults \"L1D_READS\", // L1D Cache Reads \"L1D_MISSES\" // L1D Cache Read Misses }; void initialize_perf() { // Define the events to monitor memset(pe, 0, sizeof(struct perf_event_attr) * MAX_EVENTS); for (int i = 0; i \u003c MAX_EVENTS; i++) { pe[i].size = sizeof(struct perf_event_attr); pe[i].disabled = 1; pe[i].exclude_kernel = 0; pe[i].exclude_hv = 1; pe[i].exclude_idle = 1; pe[i].exclude_user = 0; pe[i].pinned = 1; } // CPU cycles pe[0].type = PERF_TYPE_HARDWARE; pe[0].config = PERF_COUNT_HW_CPU_CYCLES; // User-level instructions pe[1].type = PERF_TYPE_HARDWARE; pe[1].config = PERF_COUNT_HW_INSTRUCTIONS; pe[1].exclude_kernel = 1; pe[1].exclude_user = 0; // Kernel-level instructions pe[2].type = PERF_TYPE_HARDWARE; pe[2].config = PERF_COUNT_HW_INSTRUCTIONS; pe[2].exclude_kernel = 0; pe[2].exclude_user = 1; // Page faults pe[3].type = PERF_TYPE_SOFTWARE; pe[3].config = PERF_COUNT_SW_PAGE_FAULTS; // L1D Cache Reads pe[4].type = PERF_TYPE_HW_CACHE; pe[4].config = PERF_COUNT_HW_CACHE_L1D | (PERF_COUNT_HW_CACHE_OP_READ \u003c\u003c 8) | (PERF_COUNT_HW_CACHE_RESULT_ACCESS \u003c\u003c 16); // L1D Cache Read Misses pe[5].type = PERF_TYPE_HW_CACHE; pe[5].config = PERF_COUNT_HW_CACHE_L1D | (PERF_COUNT_HW_CACHE_OP_READ \u003c\u003c 8) | (PERF_COUNT_HW_CACHE_RESULT_MISS \u003c\u003c 16); // Open the events for (int i = 0; i \u003c MAX_EVENTS; i++) { fd[i] = perf_event_open(\u0026pe[i], 0, CORE_NO, -1, 0); if (fd[i] == -1) { fprintf(stderr, \"Error opening event %d: %s\\n\", i, strerror(errno)); exit(EXIT_FAILURE); } } } long perf_event_open(struct perf_event_attr *hw_event, pid_t pid, int cpu, int group_fd, unsigned long flags) { return syscall(__NR_perf_event_open, hw_event, pid, cpu, group_fd, flags); } void read_perf(long long values[]) { for (int j = 0; j \u003c MAX_EVENTS; j++) { if (read(fd[j], \u0026values[j], sizeof(uint64_t)) == -1) { perror(\"Error reading counter value\"); exit(EXIT_FAILURE); } } } void write_perf(FILE *file, long long values[]) { for (int j = 0; j \u003c MAX_EVENTS; j++) { fprintf(file, \"%s,\", event_names[j]); } fprintf(file, \"\\n\"); for (int j = 0; j \u003c MAX_EVENTS; j++) { fprintf(file, \"%llu,\", values[j]); } fprintf(file, \"\\n\"); } void start_perf() { for (int j = 0; j \u003c MAX_EVENTS; j++) { ioctl(fd[j], PERF_EVENT_IOC_RESET, 0); ioctl(fd[j], PERF_EVENT_IOC_ENABLE, 0); } } void stop_perf() { for (int j = 0; j \u003c MAX_EVENTS; j++) { if (ioctl(fd[j], PERF_EVENT_IOC_DISABLE, 0) == -1) { perror(\"Error disabling counter\"); exit(EXIT_FAILURE); } } }","rdtsc#RDTSC":"On the x86 architecture, the RDTSC (Read Time-Stamp Counter) instruction is a low-level way to measure the number of CPU ticks that have elapsed since the last reset. It provides a raw metric of code execution time, making it suitable for performance profiling.\nThe RDTSC instruction measures ticks that increment at a constant rate, regardless of CPU frequency scaling (e.g., Turbo Boost, power-saving states). The number of ticks per unit of real time will remain constant, even if the core’s clock speed changes.\nHowever, it is well-known that RDTSC does not provide accurate measurements in cases of code cross-contamination due to out-of-order execution. A white paper by Intel explains how to measure ticks accurately using a combination of CPUID, RDTSC, and RDTSCP instructions. You can find the white paper here: How to Benchmark Code Execution Times on Intel® IA-32 and IA-64 Instruction Set Architectures. RDTSCP mitigates some of the out-of-order execution issues by serializing the instruction stream before reading the time-stamp counter.\nThe reason of using CPUID instruction (which generates an interrupt) before and after RDTSC/RDTSCP is to serialize the instruction stream, ensuring that all previous instructions have completed before reading the time-stamp counter. This helps to get a more accurate measurement of the code segment.\nstatic inline unsigned long long measure_rdtsc_start() { unsigned cycles_low, cycles_high; unsigned long long ticks; asm volatile(\"CPUID\\n\\t\" \"RDTSC\\n\\t\" \"mov %%edx, %0\\n\\t\" \"mov %%eax, %1\\n\\t\" : \"=r\"(cycles_high), \"=r\"(cycles_low)::\"%rax\", \"%rbx\", \"%rcx\", \"%rdx\"); ticks = (((unsigned long long)cycles_high \u003c\u003c 32) | cycles_low); return ticks; } // Inline function for measuring rdtscp ticks static inline unsigned long long measure_rdtscp_end() { unsigned cycles_low, cycles_high; unsigned long long ticks; asm volatile(\"RDTSCP\\n\\t\" \"mov %%edx, %0\\n\\t\" \"mov %%eax, %1\\n\\t\" \"CPUID\\n\\t\" : \"=r\"(cycles_high), \"=r\"(cycles_low)::\"%rax\", \"%rbx\", \"%rcx\", \"%rdx\"); ticks = (((unsigned long long)cycles_high \u003c\u003c 32) | cycles_low); return ticks; }","sample-output#Sample Output":"Starting sum_of_two_arrays... sum = 9890123 CPU_CYCLES,USER_INSNS,KERN_INSNS,PAGE_FAULTS,L1D_READS,L1D_MISSES, 298234,502345,0,0,200456,12345, Starting sum_of_two_arrays_merged_loop... sum = 9890123 CPU_CYCLES,USER_INSNS,KERN_INSNS,PAGE_FAULTS,L1D_READS,L1D_MISSES, 198456,301234,0,0,100234,6789, Now you can see exactly why the merged loop is faster—fewer L1D cache reads and fewer cache misses. The merged loop accesses array1[i] and array2[i] together in the same iteration, which has better cache locality than iterating through each array separately.","summary#Summary":"In conclusion, accurately measuring code performance requires a combination of tools and techniques. By understanding the strengths and weaknesses of each method, you can gain valuable insights into your code’s performance and identify areas for optimization.\nMethod What it measures When to use clock_gettime Wall clock time General benchmarking getrusage CPU time only Isolate from I/O wait RDTSC/RDTSCP CPU cycles Sub-µs precision, cache analysis perf / perf_event_open Hardware counters Cache misses, branch prediction, pinpointing bottlenecks Don’t forget: warm up your caches, run multiple iterations, and report confidence intervals. A single measurement number is almost meaningless without this context.","usage#Usage":"struct timespec start, end; printf(\"Measuring time using clock_gettime (timespec)...\\n\"); start = get_timespec(); long long sum = 0; for (int i = 0; i \u003c 10000; i++) { sum += sum_of_two_arrays(array1, array2, N); } end = get_timespec(); long double time_taken = diff_timespec_us(start, end); printf(\"done!\\n\"); printf(\"sum = %lld\\n\", sum); printf(\"Time taken: %Lf microseconds\\n\", time_taken);","usage-1#Usage":"long double t0, t1, time_taken; printf(\"Measuring time using rusage...\\n\"); t0 = cputime(); long long sum = 0; for (int i = 0; i \u003c 10000; i++) { sum += sum_of_two_arrays(array1, array2, N); } t1 = cputime(); time_taken = t1 - t0; printf(\"done!\\n\"); printf(\"sum = %lld\\n\", sum); printf(\"Time taken: %Lf microseconds\\n\", time_taken);","usage-2#Usage":"unsigned long long start_ticks, end_ticks, ticks_taken; printf(\"Measuring time using rdtsc...\\n\"); start_ticks = measure_rdtsc_start(); long long sum = 0; for (int i = 0; i \u003c 10000; i++) { sum += sum_of_two_arrays(array1, array2, N); } end_ticks = measure_rdtscp_end(); ticks_taken = end_ticks - start_ticks; printf(\"done!\\n\"); printf(\"sum = %lld\\n\", sum); printf(\"Ticks taken: %llu\\n\", ticks_taken); // Converting ticks to microseconds: 1 tick = 1/2.8 GHz = 0.357 ns = 0.000357 us long double time_taken = ticks_taken * 0.000357; printf(\"Time taken: %Lf microseconds\\n\", time_taken);","usage-example#Usage Example":"Here’s how I use these macros in practice to benchmark a function using all three methods:\n#include #include #include #include \"mytime.h\" int main() { long double time_taken; unsigned long niter; double ops_per_sec; // Allocate and initialize arrays int N = 100000; int *array1 = (int *)malloc(N * sizeof(int)); int *array2 = (int *)malloc(N * sizeof(int)); srand(time(NULL)); for (int i = 0; i \u003c N; i++) { array1[i] = rand() % 100; array2[i] = rand() % 100; } // Benchmark using rusage printf(\"Calibrating time using rusage...\\n\"); TIME_RUSAGE(time_taken, sum_of_two_arrays(array1, array2, N)); printf(\"Calibrated time: %Lf microseconds\\n\", time_taken); niter = 1 + (unsigned long)(1e7 / time_taken); printf(\"Performing %lu iterations\\n\", niter); long double t0 = cputime(); for (int i = 0; i \u003c niter; i++) { sum_of_two_arrays(array1, array2, N); } long double t1 = cputime() - t0; ops_per_sec = (1e6 * niter) / t1; printf(\"RESULT: %.2f operations per second\\n\", ops_per_sec); return 0; } Whenever I report timing numbers, I prefer to run the GMPBench style averaging strategy for 20-30 times and report the 95% CI mean of the execution time or throughput numbers.","usage-example-comparing-two-implementations#Usage Example: Comparing Two Implementations":"Here’s how you can use this to compare our two array sum implementations:\n// To compile: gcc -o sample2 sample_perf_util2.c perf_utils.c -I. // To run: sudo ./sample2 #include #include #include #include \"perf_utils.h\" int sum_of_two_arrays(int *array1, int *array2, int size) { int total_sum = 0; int sum1 = 0, sum2 = 0; for (int i = 0; i \u003c size; i++) { sum1 += array1[i]; } for (int i = 0; i \u003c size; i++) { sum2 += array2[i]; } total_sum = sum1 + sum2; return total_sum; } int sum_of_two_arrays_merged_loop(int *array1, int *array2, int size) { int total_sum = 0; for (int i = 0; i \u003c size; i++) { total_sum += array1[i] + array2[i]; } return total_sum; } int main() { long long values[MAX_EVENTS]; // Initialize performance monitoring initialize_perf(); int N = 100000; int *array1 = (int *)malloc(N * sizeof(int)); int *array2 = (int *)malloc(N * sizeof(int)); srand(time(NULL)); for (int i = 0; i \u003c N; i++) { array1[i] = rand() % 100; array2[i] = rand() % 100; } printf(\"\\nStarting sum_of_two_arrays...\\n\"); // Start performance monitoring start_perf(); int x = sum_of_two_arrays(array1, array2, N); // Stop performance monitoring stop_perf(); printf(\"sum = %d\\n\", x); // Read and print performance counters read_perf(values); write_perf(stdout, values); printf(\"\\nStarting sum_of_two_arrays_merged_loop...\\n\"); // Start performance monitoring for second function start_perf(); int y = sum_of_two_arrays_merged_loop(array1, array2, N); // Stop performance monitoring stop_perf(); printf(\"sum = %d\\n\", y); // Read and print performance counters read_perf(values); write_perf(stdout, values); return 0; }","wall-clock-time-clock_gettime#Wall Clock Time (clock_gettime)":"The most popular way to measure code performance is by using wall clock time. The clock_gettime function in Linux provides wall-clock time measurements with nanosecond precision. It can be used to measure the elapsed time for a specific code segment.\nWhile there are multiple clock types available, CLOCK_MONOTONIC (or CLOCK_MONOTONIC_RAW) is generally preferred for measuring elapsed time as it is not affected by system time changes. Unlike CLOCK_REALTIME, it won’t jump backwards if NTP adjusts the system clock mid-measurement. To use clock_gettime, we have to include the header file.\nHere are some helper functions I use for measuring time using clock_gettime:\n#include // function to get the timespec stamp static inline struct timespec get_timespec() { struct timespec ts; clock_gettime(CLOCK_MONOTONIC_RAW, \u0026ts); return ts; } // function to compute the difference between two timespec stamps in microseconds static inline long diff_timespec_us(struct timespec start, struct timespec end) { struct timespec temp; if ((end.tv_nsec - start.tv_nsec) \u003c 0) { temp.tv_sec = end.tv_sec - start.tv_sec - 1; temp.tv_nsec = 1000000000 + end.tv_nsec - start.tv_nsec; } else { temp.tv_sec = end.tv_sec - start.tv_sec; temp.tv_nsec = end.tv_nsec - start.tv_nsec; } // return in microseconds return (temp.tv_sec * 1000000000 + temp.tv_nsec) / 1000; }","why-warmups-matter-the-cold-cache-problem#Why Warmups Matter: The Cold Cache Problem":"This is something that tripped me up early on. When you run a function for the first time, the data it accesses is not in the CPU cache—it has to be fetched from main memory. These are called “cold cache misses,” and they are slow. Memory access latency can be 100-300 cycles, while an L1 cache hit is just a few cycles.\nHere’s the problem: if you measure your code without warming up the cache first, the memory access time dominates the measurement. You might optimize your CPU computations and see no improvement in your benchmarks—because the memory latency was hiding the CPU time all along. This is especially frustrating when you’ve worked hard on an optimization and the numbers tell you it didn’t help.\n// BAD: First run has cold cache start = clock(); result = sum_of_two_arrays(array1, array2, N); // Cache misses everywhere! end = clock(); // This measures memory latency, not CPU performance // GOOD: Warm up the cache first for (int i = 0; i \u003c 3; i++) { volatile int r = sum_of_two_arrays(array1, array2, N); (void)r; } // Now the data is in cache start = clock(); result = sum_of_two_arrays(array1, array2, N); // Cache hits, measures actual compute end = clock(); If you’re comparing two implementations and one happens to run first (cold cache) while the other runs second (warm cache), you’ll get misleading results. Always warm up before measuring, or measure both with equally cold caches."},"title":"How to Measure Performance of Your Code"},"/news/abhishek-google-india-research-award-2024/":{"data":{"":"We are thrilled to announce that Prof. Abhishek Bichhawat has been awarded the Google India Research Award 2024 primarily for the work “Towards a Secure and Private Web Browsing Experience”. Kudos to the team!"},"title":"Google India Research Award 2024 goes to Prof. Abhishek Bichhawat!"},"/news/gayatri-google-phd-fellowship/":{"data":{"":"We are thrilled to announce that Gayatri Priyadarsini Kancherla has been awarded the Google PhD Fellowship 2025 in the area of Privacy, Safety, and Security. Congratulations to Gayatri for this prestigious recognition!\nFor more details, visit: https://research.google/programs-and-events/phd-fellowship/recipients/"},"title":"Gayatri got the Google PhD Fellowship 2025!"},"/news/gayatri-pets25/":{"data":{"":"We are delighted to announce that Gayatri’s work “Johnny can’t revoke consent either: measuring compliance of consent revocation on the web” has been published at the Privacy Enhancing Technologies Symposium 2025."},"title":"Gayatri presented her work at PETS 2025"},"/news/gayatri-web-conf25/":{"data":{"":"We are delighted to announce that Gayatri’s work “Least Privilege Access for Persistent Storage Mechanisms in Web Browsers” has been published at the Web Conference 2025."},"title":"Gayatri's Work Accepted at WWW 2025"},"/people/":{"data":{"":"Faculty Abhishek Bichhawat Assistant Professor\nFormal Methods \u0026 Verification | Web Security | Usable Security Abhishek Bichhawat is an Assistant Professor in the Department of Computer Science and Engineering at IIT Gandhinagar. Previously, he spent three years as a postdoctoral fellow at Cylab, Carnegie Mellon University after he completed his PhD working with Christian Hammer and Deepak Garg at Saarland University, Germany where he was affiliated with CISPA and the International Max Planck Research School for Computer Science. Prior to that he completed his Masters from Indian Insitute of Technology Roorkee and his Bachelors from Anna University. Before joining the PhD program, he worked as a Member of Technical Staff at Oracle India Pvt. Ltd. in Bengaluru, India. Ph.D. Gayatri Priyadarsini Kancherla 5th Year Ph.D.\nJuly 2021 - Present\nWeb Privacy | Browser Security | Usable Consent Sreyashi Karmakar 2nd Year Ph.D.\nJuly 2024 - Present\nBlockchain Security | Post-Quantum Cryptography | Zero-Knowledge Proofs Subhrajit Das 1st Year Ph.D.\nJuly 2023 - Present\nCPU Parallelism | Distributed Systems | Usable Security M.Tech. Past Members"},"title":"People"},"/projects/":{"data":{"":"To be added."},"title":"Projects"},"/publications/":{"data":{"":"","2010#2010":"A survey on issues in mobile grid computing A Bichhawat, RC Joshi\nInt J Recent Trends Eng. Technol 4 (2), 2010","2011#2011":"Security architecture for virtual machines U Tupakula, V Varadharajan, A Bichhawat\nInternational Conference on Algorithms and Architectures for Parallel …, October 2011\nProactive Fault Tolerance Technique for a Mobile Grid Environment A Bichhawat, RC Joshi\nInternational Conference on Advances in Computing and Communication, 2011","2014#2014":"Information flow control in WebKit’s JavaScript bytecode A Bichhawat, V Rajani, D Garg, C Hammer\nInternational conference on principles of security and trust, April 2014\nGeneralizing permissive-upgrade in dynamic information flow analysis A Bichhawat, V Rajani, D Garg, C Hammer\nProceedings of the Ninth Workshop on Programming Languages and Analysis for Security, July 2014\nException handling for dynamic information flow control A Bichhawat Companion Proceedings of the 36th International Conference on Software …, May 2014","2015#2015":"Information flow control for event handling and the DOM in web browsers V Rajani, A Bichhawat, D Garg, C Hammer\n2015 IEEE 28th Computer Security Foundations Symposium, 2015\nPost-dominator analysis for precisely handling implicit flows A Bichhawat\nIEEE/ACM 37th IEEE International Conference on Software Engineering, May 2015","2017#2017":"WebPol: Fine-grained information flow policies for web browsers A Bichhawat, V Rajani, J Jain, D Garg, C Hammer European Symposium on Research in Computer Security, 2017\nPractical dynamic information flow control A Bichhawat Saarländische Universitäts-und Landesbibliothek, 2017","2020#2020":"Contextual and granular policy enforcement in database-backed applications A Bichhawat, M Fredrikson, J Yang, A Trehan\nProceedings of the 15th ACM Asia Conference on Computer and Communications Security, 2020\nFirst-order Gradual Information Flow Types with Gradual Guarantees A Bichhawat, MK McCall, L Jia\narXiv preprint arXiv:2003.12819, 2020","2021#2021":"An in-depth symbolic security analysis of the ACME standard Karthikeyan Bhargavan, Abhishek Bichhawat, Quoc Huy Do, Pedram Hosseyni, Ralf Küsters, Guido Schmitz, Tim Würtele\nProceedings of the 2021 ACM SIGSAC conference on computer and communications security, November 2021\nA Tutorial-Style Introduction to DY* Karthikeyan Bhargavan, Abhishek Bichhawat, Quoc Huy Do, Pedram Hosseyni, Ralf Küsters, Guido Schmitz, Tim Würtele\nProtocols, Strands, and Logic: Essays Dedicated to Joshua Guttman on the Occasion of his 66.66 th Birthday, November 2021\nDY*: A Modular Symbolic Verification Framework for Executable Cryptographic Protocol Code Karthikeyan Bhargavan, Abhishek Bichhawat, Quoc Huy Do, Pedram Hosseyni, Ralf Küsters, Guido Schmitz, Tim Würtele\n2021 IEEE European Symposium on Security and Privacy (EuroS\u0026P), September 2021\nSAFETAP: An efficient incremental analyzer for trigger-action programs McKenna McCall, Faysal Hossain Shezan, Abhishek Bichhawat, Camille Cobb, Limin Jia, Yuan Tian, Cooper Grace, Mitchell Yang\nCarnegie Mellon University, 2021\nGradual security types and gradual guarantees A Bichhawat, MK McCall, L Jia\n2021 IEEE 34th Computer Security Foundations Symposium (CSF), June 2021\nPermissive runtime information flow control in the presence of exceptions A Bichhawat, V Rajani, D Garg, C Hammer\nJournal of Computer Security 29 (4), 2021\nDY⋆ Code Repository Karthikeyan Bhargavan, Abhishek Bichhawat, Quoc Huy Do, Pedram Hosseyni, Ralf Küsters, Guido Schmitz, Tim Würtele\nURL: https://github. com/reprosec/dolev-yao-star, 2021\nAutomating Audit with Policy Inference A Bichhawat, M Fredrikson, J Yang\n2021 IEEE 34th Computer Security Foundations Symposium (CSF), June 2021","2022#2022":"Compositional information flow monitoring for reactive programs MK McCall, A Bichhawat, L Jia 2022 IEEE 7th European Symposium on Security and Privacy (EuroS\u0026P), June 2022\nNoise: A library of verified high-performance secure channel protocol implementations S Ho, J Protzenko, A Bichhawat, K Bhargavan 2022 IEEE Symposium on Security and Privacy (SP), May 2022","2023#2023":"Tainted Secure Multi-Execution to Restrict Attacker Influence MK McCall, A Bichhawat, L Jia Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security, November 2023\nLayered Symbolic Security Analysis in Karthikeyan Bhargavan, Abhishek Bichhawat, Pedram Hosseyni, Ralf Küsters, Klaas Pruiksma, Guido Schmitz, Clara Waldmann, Tim Würtele European Symposium on Research in Computer Security, September 2023\nTowards Usable Security Analysis Tools for {Trigger-Action} Programming McKenna McCall, Eric Zeng, Faysal Hossain Shezan, Mitchell Yang, Lujo Bauer, Abhishek Bichhawat, Camille Cobb, Limin Jia, Yuan Tian Nineteenth Symposium on Usable Privacy and Security (SOUPS 2023), August 2023","2024#2024":"Online Authentication Habits of Indian Users P Choudhary, S Das, MP Potta, P Das, A Bichhawat 2024 Conference on Building a Secure \u0026 Empowered Cyberspace (BuildSEC), December 2024\nWeb Privacy Perceptions Amongst Indian Users G Priyadarsini, A Saxena, A Dey, Prakriti, A Bichhawat International Conference on Information Systems Security, December 2024","2025#2025":"Fall-through Semantics for Mitigating Timing-based Side Channel Leaks\nAniket Mishra and Abhishek Bichhawat\n45th IARCS Annual Conference on Foundations of Software Technology and Theoretical Computer Science (FSTTCS), December 2025\nJohnny can’t revoke consent either: measuring compliance of consent revocation on the web GP Kancherla, N Bielova, C Santos, A Bichhawat Proceedings on Privacy Enhancing Technologies, July 2025\nHow Usable is Consent Withdrawal on the Web? UI Requirements and Expert Evaluation S Ahuja, GP Kancherla, CT Santos, N Bielova, A Bichhawat HAL open science, 2025\nOn the Prevalence and Usage of Commit Signing on GitHub: A Longitudinal and Cross-Domain Study A Sharma, S Karmakar, GP Kancherla, A Bichhawat Evaluation and Assessment in Software Engineering (EASE) 2025, June 2025\nLeast Privilege Access for Persistent Storage Mechanisms in Web Browsers GP Kancherla, D Goel, A Bichhawat Proceedings of the ACM on Web Conference 2025, April 2025\nA Unified Browser-Based Consent Management Framework GP Kancherla, A Bichhawat 47th International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER), April 2025\nWorst-Case Response Time Analysis for Periodic Programs with Nested Locks HM Ramolia, SS Kanawade, A Bichhawat Proceedings of the 18th Innovations in Software Engineering Conference, February 2025","2026#2026":"“I Wonder if These Warnings Are Accurate”: Security and Privacy Advice in Nine Majority World Countries\nCollins W. Munyendo, Veronica A. Rivera, Jackie Hu, Emmanuel Tweneboah, Amna Shahnawaz, Karen Sowon, Dilara Kekulluoglu, Marcos Silva, Yue Deng, Mercy Omeiza, Gayatri Priyadarsini Kancherla, Maria Rosario Niniz Silva, Maryam Mustafa, Abhishek Bichhawat, Francisco Marmolejo-Cossio, Elissa M. Redmiles, Yixin Zou\n47th IEEE Symposium on Security and Privacy (Oakland), May 2026","misc#Misc.":"Restricting Attacker Influence in Reactive Programs with Dynamic Secrets MK McCall, A Bichhawat, L Jia\nCarnegie Mellon University\nPoster: Security in Web-Based Workflows Thomas Bauereiß, Abhishek Bichhawat, Iulia Bolosteanu, Peter Faymonville, Bernd Finkbeiner, Deepak Garg, Richard Gay, Sergey Grebenshchikov, Christian Hammer, Dieter Hutter, Ondrej Kuncar, Peter Lammich, Heiko Mantel, Christian Müller, Andrei Popescu, Markus Rabe, Vineet Rajani, Helmut Seidl, Markus Tasch, Leander Tentrup\nStatus Report: Formal Analysis of Web Security K Bhargavan, A Bichhawat, QH Do, D Fett, R Küsters, G Schmitz"},"title":"Publications"}}